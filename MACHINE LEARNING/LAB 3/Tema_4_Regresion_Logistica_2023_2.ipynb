{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svopgWO7bExV"
      },
      "source": [
        "## Logistic Regression Practice: Breast Cancer\n",
        " ----\n",
        "  University  : UTEC \\\\\n",
        "Course      : Machine Learning \\\\\n",
        "Professor   : Cristian López Del Alamo \\\\\n",
        "Topic      : Logistic Regression \\\\\n",
        "  \n",
        " ----\n",
        "\n",
        "\n",
        " Integrantes:\n",
        " - 1.  Chavez Balarezo, Fabricio(100 $\\; \\% \\;$)\n",
        " - 2.  Soto Mayta, Benjamin(100 $\\; \\% \\;$)\n",
        " - 3.  Vilchez Espinoza, Fabrizzio(100 $\\; \\% \\;$)\n",
        " - 4.  ($\\; \\% \\;$)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiMdtNbVlMTd"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The dataset contains cases from a study conducted between 1958 and 1970 at the Billings Hospital of the University of Chicago regarding the survival of patients who had undergone surgery for breast cancer.\n",
        "\n",
        "The database consists of 306 objects, each object has 3 features (Patient's age at the time of the operation, Years of operation, and Number of positive axillary nodes detected) and one predictor (variable to predict survival status, 1 if the patient lived, 0 if the patient died).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<font color=\"blue!10\">**Your task is to predict whether a patient will survive or not.**\n",
        "</font>\n",
        "\n",
        "\n",
        "## <font color=\"#FF6666\"> Remember,  for every mistake your model makes, a possible death will weight on the shoulders of your team.</font>\n",
        "\n",
        "**Download the Database**: [Click](https://docs.google.com/spreadsheets/d/137VWC-uXIeWUIy5F2oVkfPqicFeNFU5oUWVsh5om9a4/edit?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UDEHxYo2iDYq"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF1-EUpeaGvZ"
      },
      "source": [
        "\n",
        "\n",
        "1. **Hypothesis**:\n",
        "\n",
        "-  Line Equation or Hyperplane\n",
        "\\begin{equation}\n",
        "h(x_i) = w_0 + w_1x_{i1} +  w_2x_{i2} ... w_kx_{ik}\n",
        "\\end{equation} \\\\\n",
        "\n",
        "- In matrix form\n",
        "\n",
        "\\begin{equation}\n",
        "h(x_i) = XW^t\n",
        "\\end{equation} \\\\\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Sigmoid Function Equation (Binary Classifier)\n",
        "\\begin{equation}\n",
        "s(x_i) = \\frac{1}{1 + e^{-h(x)}}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Note: **Remember that X is a matrix where the columns represent the features, and the rows represent the number of elements in the database. Don’t forget to add a column of all 1s at the beginning of matrix X for the bias**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IccO1C-1b6gH"
      },
      "outputs": [],
      "source": [
        "def h(x, w):\n",
        "    return np.dot(x, w.T)\n",
        "\n",
        "def S(x, w):\n",
        "    return 1 / (1 + np.exp(-h(x, w)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsEclK8ZcDAv"
      },
      "source": [
        "2  **Loss Function** (binary Cross-Entropy)\n",
        "\n",
        "\\begin{equation}\n",
        "L = -\\frac{1}{n}\\sum_{i=0}^n(y_ilog(s(x_i)) + (1-y_i)log(1-s(x_i)))  \n",
        "\\end{equation} \\\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KDeZtXSecvnb"
      },
      "outputs": [],
      "source": [
        "def Loss(y, y_aprox):\n",
        "    e = 1e-10\n",
        "    y_aprox = np.clip(y_aprox, e, 1 - e)\n",
        "    return -np.sum(y * np.log(y_aprox) + (1 - y) * np.log(1 - y_aprox))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5e7Tbc_c7Y8"
      },
      "source": [
        "3 **Derivatives**\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial L}{\\partial w_j} = \\frac{1}{n}\\sum_{i=0}^n(y_i - s(x_i))(-x_{ij})\n",
        "\\end{equation} \\\\\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Note: $x_{ij}$ refers to the $j_{esima}$ characteristic of the $i_{esimo}$    object in the training dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "g4cD0fHuc6_I"
      },
      "outputs": [],
      "source": [
        "def Derivatives(x, y, w):\n",
        "    y_aprox = S(x, w)\n",
        "    return np.dot((y_aprox - y).T, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_YvV9kAfQFL"
      },
      "source": [
        "4  Change parameters\n",
        "\n",
        "\\begin{equation}\n",
        " w_j = w_j - \\alpha\\frac{\\partial L}{\\partial w_j}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sN6Mj8G_fnPA"
      },
      "outputs": [],
      "source": [
        "def change_parameters(w, derivatives, alpha):\n",
        "    return w - alpha * derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWEJPUGWfykV"
      },
      "source": [
        "## Bringing it all together : **Training** code\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-4iDI_owgkIV"
      },
      "outputs": [],
      "source": [
        "def training(x,y, epochs, alpha):\n",
        "  L_value = []\n",
        "  n_feactures = x.shape[1]\n",
        "  w = np.array([np.random.rand() for i in range(n_feactures)])\n",
        "  for i in range(epochs):\n",
        "    y_aprox = S(x,w)  # Use S(x,w) instead of h(x,w)\n",
        "    L =  Loss(y,y_aprox)\n",
        "    dw = Derivatives(x,y,w)\n",
        "    w =  change_parameters(w, dw, alpha)\n",
        "    L_value.append(L)\n",
        "  return L_value,w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17O_Nrq5g3Ma"
      },
      "source": [
        "## **Testing** code\n",
        "Modify this function to return true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- TP: Percentage of elements predicted as class 1 (positive) that are actually class 1 (positive).\n",
        "- TN: Percentage of elements predicted as class 0 (negative) and that are actually class 0 (negative).\n",
        "- FP: Percentage of elements predicted as class 1 (positive) and that are actually class 0 (negative).\n",
        "- FN: Percentage of elements predicted as class 0 (negative) and that are actually class 1 (positive).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_Y7UDBtBg2cs"
      },
      "outputs": [],
      "source": [
        "def Testing(x_test, y_test,w):\n",
        "   n = len(y_test)\n",
        "   y_pred = []\n",
        "   for i in range(n):\n",
        "     y_pred.append(S(x_test,w))\n",
        "   print(\"Number of correct data : \", sum(y_pred == y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwTbWgnGghfv"
      },
      "source": [
        "\n",
        "# 1 Loading Database\n",
        "-  Don't forget to normalize data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IgnwNHBJkDgI",
        "outputId": "06528b82-d384-4e74-b5ea-69986c5e1e52"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "filename = {\"db.csv\": \"db.csv\"}\n",
        "name = list(filename.keys())[0]\n",
        "data = pd.read_csv(name)\n",
        "X = data[[\"C1\",\"C2\",\"C3\"]]\n",
        "Y = data[[\"Clase\"]]\n",
        "X.head()\n",
        "\n",
        "\n",
        "\n",
        "#  normalize data: write your code here\n",
        "\n",
        "X = (X - X.min()) / (X.max() - X.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JbZfoyMmvXm"
      },
      "source": [
        "\n",
        "*   Randomly splitting data for training and testing\n",
        "*   Training 70$\\%$\n",
        "*   Training 30$\\%$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wpVQjUyT3DYp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , random_state=104,  test_size=0.30,    shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erTOiXnZn4nf"
      },
      "source": [
        "# 2. Training the Model\n",
        "\n",
        "- Modify hyperparameters for better results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1wgLPQWw3WbF"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      5\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 6\u001b[0m L,W \u001b[38;5;241m=\u001b[39m training(X_train, Y_train, epochs, alpha)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plotting the Loss Function vs Epochs\u001b[39;00m\n\u001b[0;32m      8\u001b[0m epochs_list \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs)]\n",
            "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 1000\n",
        "alpha = 0.001\n",
        "L,W = training(X_train, Y_train, epochs, alpha)\n",
        "# Plotting the Loss Function vs Epochs\n",
        "epochs_list = [i for i in range(epochs)]\n",
        "plt.plot(epochs_list ,L)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urBVFBIMpfvg"
      },
      "source": [
        "# 3. Testing the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UDp9eocpyy0"
      },
      "outputs": [],
      "source": [
        " Testing(X_test, Y_test,W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th-ybXehkbAa"
      },
      "source": [
        "## Develop the following activities:\n",
        "- Implement all the necessary functions for the code to work correctly.\n",
        "- Try to use matrix operations to make your code more efficient.\n",
        "-  Find the best hyperparameters.\n",
        "- Calculate the percentage of patients correctly belonging to class 1 (true positives).\n",
        "- Calculate the percentage of patients correctly belonging to class 0 (true negatives).\n",
        "- Calculate the percentage of patients incorrectly belonging to class 1 (false positives).\n",
        "- Calculate the percentage of patients incorrectly belonging to class 0 (false negatives).\n",
        "- Show a table with true positives, true negatives, false positives, and false negatives\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnIBN6QAqOmG"
      },
      "source": [
        "A very good book: [click](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)\n",
        "\n",
        "Tips for manipulate pandas : https://towardsdatascience.com/python-pandas-vs-r-dplyr-5b5081945ccb\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
