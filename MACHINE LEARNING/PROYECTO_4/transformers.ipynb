{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "                                             message label\n",
      "0  I saw this movie in NEW York city. I was waiti...   neg\n",
      "1  This is a German film from 1974 that is someth...   neg\n",
      "2  I attempted watching this movie twice and even...   neg\n",
      "3  On his birthday a small boys tells his mother ...   neg\n",
      "4  The person who wrote the review \"enough with t...   pos\n",
      "Index(['message', 'label'], dtype='object')\n",
      "Train texts: 20000, Validation texts: 5000\n",
      "Train labels: 20000, Validation labels: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/3750 [03:57<?, ?it/s]\n",
      "c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 13%|‚ñà‚ñé        | 500/3750 [00:59<06:22,  8.50it/s]\n",
      " 13%|‚ñà‚ñé        | 500/3750 [00:59<06:22,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4569, 'grad_norm': 7.401895046234131, 'learning_rate': 5e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 780/3750 [01:33<05:42,  8.66it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m\n\u001b[0;32m     68\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     69\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# El modelo a entrenar\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# Argumentos de entrenamiento\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(p\u001b[38;5;241m.\u001b[39mlabel_ids, p\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))},  \u001b[38;5;66;03m# M√©trica de precisi√≥n\u001b[39;00m\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[0;32m     80\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\transformers\\trainer.py:2270\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2274\u001b[0m ):\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Comprobar si CUDA est√° disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Verificar las columnas del DataFrame\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# Convertir las etiquetas a valores num√©ricos\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Restablecer los √≠ndices de los conjuntos de datos\n",
    "train_texts = train_texts.reset_index(drop=True)\n",
    "val_texts = val_texts.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "val_labels = val_labels.reset_index(drop=True)\n",
    "\n",
    "# Verificar los tama√±os de los conjuntos de datos\n",
    "print(f\"Train texts: {len(train_texts)}, Validation texts: {len(val_texts)}\")\n",
    "print(f\"Train labels: {len(train_labels)}, Validation labels: {len(val_labels)}\")\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenizar los textos\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Crear un dataset personalizado en PyTorch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Asegurarse de que las etiquetas sean LongTensor\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "\n",
    "# Definir los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Carpeta de salida para los resultados\n",
    "    num_train_epochs=3,              # N√∫mero de √©pocas de entrenamiento\n",
    "    per_device_train_batch_size=16,  # Tama√±o del batch para entrenamiento\n",
    "    per_device_eval_batch_size=64,   # Tama√±o del batch para evaluaci√≥n\n",
    "    warmup_steps=500,                # Pasos de calentamiento para el scheduler\n",
    "    weight_decay=0.01,               # Peso de decaimiento\n",
    "    logging_dir='./logs',            # Carpeta de salida para los logs\n",
    "    evaluation_strategy=\"epoch\",     # Estrategia de evaluaci√≥n en cada √©poca\n",
    ")\n",
    "\n",
    "# Crear el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # El modelo a entrenar\n",
    "    args=training_args,                  # Argumentos de entrenamiento\n",
    "    train_dataset=train_dataset,         # Dataset de entrenamiento\n",
    "    eval_dataset=val_dataset,            # Dataset de validaci√≥n\n",
    "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1))},  # M√©trica de precisi√≥n\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Generar predicciones y el reporte de clasificaci√≥n\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "accuracy = accuracy_score(val_labels, pred_labels)\n",
    "class_report = classification_report(val_labels, pred_labels, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
