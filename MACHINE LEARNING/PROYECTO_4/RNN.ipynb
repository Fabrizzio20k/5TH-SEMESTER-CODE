{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\vilch\\AppData\\Local\\Temp\\ipykernel_179092\\1477969956.py\", line 62, in <module>\n",
      "    X_train = torch.tensor(X_train, dtype=torch.long)\n",
      "C:\\Users\\vilch\\AppData\\Local\\Temp\\ipykernel_179092\\1477969956.py:62: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  X_train = torch.tensor(X_train, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos preprocesados\n",
    "train = pd.read_csv('train_clean.csv')\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenización y lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "train['clean_message'] = train['clean_message'].apply(preprocess_text)\n",
    "\n",
    "# Construir el vocabulario\n",
    "all_tokens = [token for message in train['clean_message'] for token in message]\n",
    "vocab = list(set(all_tokens))\n",
    "vocab_to_int = {word: i+1 for i, word in enumerate(vocab)}\n",
    "\n",
    "# Convertir los textos a secuencias de enteros\n",
    "def text_to_sequence(text):\n",
    "    return [vocab_to_int[word] for word in text]\n",
    "\n",
    "train['clean_message_seq'] = train['clean_message'].apply(text_to_sequence)\n",
    "\n",
    "# Padding de secuencias\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded = np.zeros((len(sequences), maxlen), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            padded[i, :maxlen] = seq[:maxlen]\n",
    "        else:\n",
    "            padded[i, :len(seq)] = seq\n",
    "    return padded\n",
    "\n",
    "maxlen = max(train['clean_message_seq'].apply(len))\n",
    "X = pad_sequences(train['clean_message_seq'], maxlen)\n",
    "\n",
    "# Codificación de etiquetas\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(train['label'])\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir a tensores\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Crear DataLoader\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, n_layers=2, dropout=0.5):\n",
    "        super(ImprovedRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers=n_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 para bidireccionalidad\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # Obtener la salida del último paso de tiempo\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "embed_size = 100\n",
    "hidden_size = 128  # Tamaño de la capa oculta\n",
    "output_size = 1\n",
    "n_layers = 2  # Número de capas recurrentes\n",
    "dropout = 0.5  # Dropout\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImprovedRNNModel(vocab_size, embed_size, hidden_size, output_size, n_layers, dropout).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.701944545173645\n",
      "Epoch 2, Loss: 0.697131266117096\n",
      "Epoch 3, Loss: 0.6958057615280151\n",
      "Epoch 4, Loss: 0.69754060754776\n",
      "Epoch 5, Loss: 0.6976643171310425\n",
      "Epoch 6, Loss: 0.6973668911933899\n",
      "Epoch 7, Loss: 0.6972004951477051\n",
      "Epoch 8, Loss: 0.6978647116661072\n",
      "Epoch 9, Loss: 0.6979470928192139\n",
      "Epoch 10, Loss: 0.6981397317886353\n",
      "Accuracy: 0.4892\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      1.00      0.66      2446\n",
      "         1.0       0.00      0.00      0.00      2554\n",
      "\n",
      "    accuracy                           0.49      5000\n",
      "   macro avg       0.24      0.50      0.33      5000\n",
      "weighted avg       0.24      0.49      0.32      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vilch\\OneDrive\\Escritorio\\5TH SEMESTER CODE\\MACHINE LEARNING\\PROYECTO_4\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Regularización L2\n",
    "\n",
    "# Entrenar el modelo\n",
    "n_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Mover datos a la GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluar el modelo\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Mover datos a la GPU\n",
    "        outputs = model(inputs)\n",
    "        y_pred.extend(outputs.squeeze().cpu().tolist())  # Mover predicciones a la CPU\n",
    "        y_true.extend(labels.cpu().tolist())  # Mover etiquetas a la CPU\n",
    "\n",
    "# Convertir predicciones a etiquetas binarias\n",
    "y_pred = [1 if pred >= 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluar el modelo\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
