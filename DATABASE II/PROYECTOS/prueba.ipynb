{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escrito block_0.txt\n",
      "Escrito block_1.txt\n",
      "Escrito block_2.txt\n",
      "Índice invertido final escrito en final_inverted_index.txt\n",
      "Top-k documentos relevantes:\n",
      "Score: 1.0, Track Name: Lost In The Rhythm - Original Mix, Artist: Jamie Berry, Album: Lost In The Rhythm\n",
      "Score: 1.0, Track Name: I'll Whip Ya Head Boy, Artist: 50 Cent, Album: Window Shopper (International Version)\n",
      "Score: 0.6655957411034531, Track Name: Cannonball, Artist: Damien Rice, Album: O\n",
      "Score: 0.533389160947312, Track Name: Parking Lot, Artist: Anderson .Paak, Album: Malibu\n",
      "Score: 0.516429629599365, Track Name: Rolling Stone, Artist: Ecko, Album: Rolling Stone\n",
      "Score: 0.454531579210013, Track Name: Sunshine Of Your Love, Artist: Cream, Album: Disraeli Gears (Deluxe Edition)\n",
      "Score: 0.4173687240152495, Track Name: Schmeckt (feat. LX), Artist: Hemso, Album: Schmeckt (feat. LX)\n",
      "Score: 0.4074020475499432, Track Name: Glad You Came, Artist: The Wanted, Album: Battleground (Deluxe Edition)\n",
      "Score: 0.4026404291827133, Track Name: Big Girls Don't Cry (Personal), Artist: Fergie, Album: Songs About Love (UMGI Version)\n",
      "Score: 0.38907546323337266, Track Name: Exposed - APEK's VIP Mix, Artist: APEK, Album: Exposed\n",
      "Score: 0.38907546323337266, Track Name: styl, Artist: Be Vis, Album: styl\n",
      "Score: 0.38907546323337266, Track Name: Welcome To Your Life, Artist: Grouplove, Album: Big Mess\n",
      "Score: 0.38907546323337266, Track Name: Groove Me, Artist: Guy, Album: Guy - Special Edition\n",
      "Score: 0.38907546323337266, Track Name: Move Over, Artist: Janis Joplin, Album: Pearl (Legacy Edition)\n",
      "Score: 0.3776904896624341, Track Name: I Still Haven't Found What I'm Looking For, Artist: U2, Album: The Joshua Tree (Super Deluxe)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import heapq\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import defaultdict, Counter\n",
    "from math import log, sqrt\n",
    "\n",
    "# Descargar los recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el stemmer y las stopwords\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Tokeniza, elimina stopwords y aplica stemming a las palabras \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    \"\"\" Calcula la frecuencia de término (TF) \"\"\"\n",
    "    tf = Counter(tokens)\n",
    "    total_terms = len(tokens)\n",
    "    for term in tf:\n",
    "        tf[term] /= total_terms\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \"\"\" Calcula la frecuencia inversa de documentos (IDF) \"\"\"\n",
    "    N = len(documents)\n",
    "    df = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        unique_terms = set(doc)\n",
    "        for term in unique_terms:\n",
    "            df[term] += 1\n",
    "    idf = {term: log(N / df[term]) for term in df}\n",
    "    return idf\n",
    "\n",
    "def compute_norm(tf_idf):\n",
    "    \"\"\" Calcula la norma de un documento \"\"\"\n",
    "    return sqrt(sum(weight**2 for weight in tf_idf.values()))\n",
    "\n",
    "def spimi_invert(documents, block_size):\n",
    "    \"\"\" Construye el índice invertido usando SPIMI \"\"\"\n",
    "    block_id = 0\n",
    "    term_dict = defaultdict(lambda: defaultdict(float))\n",
    "    doc_norms = {}\n",
    "    idf = compute_idf(documents)\n",
    "    \n",
    "    for doc_id, tokens in enumerate(documents):\n",
    "        tf = compute_tf(tokens)\n",
    "        tf_idf = {term: tf[term] * idf[term] for term in tf}\n",
    "        doc_norms[doc_id] = compute_norm(tf_idf)\n",
    "        \n",
    "        for term, weight in tf_idf.items():\n",
    "            term_dict[term][doc_id] = weight\n",
    "        \n",
    "        # Si el bloque excede el tamaño permitido, escribir a disco y limpiar\n",
    "        if len(term_dict) >= block_size:\n",
    "            write_block_to_disk(term_dict, block_id)\n",
    "            block_id += 1\n",
    "            term_dict.clear()\n",
    "    \n",
    "    if term_dict:  # Escribir el último bloque si queda algo\n",
    "        write_block_to_disk(term_dict, block_id)\n",
    "        block_id += 1\n",
    "    \n",
    "    return doc_norms, block_id\n",
    "\n",
    "def write_block_to_disk(term_dict, block_id):\n",
    "    \"\"\" Escribe un bloque de índice invertido a disco \"\"\"\n",
    "    filename = f'block_{block_id}.txt'\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for term, postings in sorted(term_dict.items()):\n",
    "            postings_list = ' '.join(f\"{doc_id}:{weight}\" for doc_id, weight in postings.items())\n",
    "            f.write(f\"{term}: {postings_list}\\n\")\n",
    "    print(f\"Escrito {filename}\")\n",
    "\n",
    "def merge_blocks(block_count, output_file):\n",
    "    \"\"\" Fusiona bloques de índices invertidos en un solo archivo \"\"\"\n",
    "    heap = []\n",
    "    file_pointers = [open(f'block_{i}.txt', 'r', encoding='utf-8') for i in range(block_count) if os.path.exists(f'block_{i}.txt')]\n",
    "    for i, fp in enumerate(file_pointers):\n",
    "        term, postings = read_next_line(fp)\n",
    "        if term:\n",
    "            heapq.heappush(heap, (term, postings, i))\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        while heap:\n",
    "            term, postings, i = heapq.heappop(heap)\n",
    "            f.write(f\"{term}: {postings}\\n\")\n",
    "            next_line = file_pointers[i].readline().strip()\n",
    "            if next_line:\n",
    "                next_term, next_postings = next_line.split(': ', 1)\n",
    "                heapq.heappush(heap, (next_term, next_postings, i))\n",
    "    \n",
    "    for fp in file_pointers:\n",
    "        fp.close()\n",
    "    print(f\"Índice invertido final escrito en {output_file}\")\n",
    "\n",
    "def read_next_line(file_pointer):\n",
    "    \"\"\" Lee la siguiente línea de un archivo y retorna el término y sus postings \"\"\"\n",
    "    line = file_pointer.readline().strip()\n",
    "    if line:\n",
    "        term, postings = line.split(': ', 1)\n",
    "        return term, postings\n",
    "    return None, None\n",
    "\n",
    "def cosine_similarity(query_vector, doc_vector, doc_norm):\n",
    "    \"\"\" Calcula la similitud de coseno entre dos vectores \"\"\"\n",
    "    dot_product = sum(query_vector[term] * doc_vector.get(term, 0) for term in query_vector)\n",
    "    return dot_product / doc_norm if doc_norm else 0.0\n",
    "\n",
    "def process_query(query, idf, doc_norms, inverted_index, data, top_k=15):\n",
    "    \"\"\" Procesa una consulta y retorna los top-k documentos relevantes con metadata \"\"\"\n",
    "    tokens = tokenize(query)\n",
    "    tf_query = compute_tf(tokens)\n",
    "    tf_idf_query = {term: tf_query[term] * idf.get(term, 0) for term in tf_query}\n",
    "    query_norm = compute_norm(tf_idf_query)\n",
    "    \n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    for term, query_weight in tf_idf_query.items():\n",
    "        if term in inverted_index:\n",
    "            for doc_id, doc_weight in inverted_index[term].items():\n",
    "                scores[doc_id] += query_weight * doc_weight\n",
    "    \n",
    "    ranked_scores = sorted(((score / (doc_norms[doc_id] * query_norm), doc_id) \n",
    "                            for doc_id, score in scores.items()), reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for score, doc_id in ranked_scores[:top_k]:\n",
    "        song_info = {\n",
    "            'score': score,\n",
    "            'track_name': data.iloc[doc_id]['track_name'],\n",
    "            'track_artist': data.iloc[doc_id]['track_artist'],\n",
    "            'track_album_name': data.iloc[doc_id]['track_album_name']\n",
    "        }\n",
    "        results.append(song_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Leer dataset desde un archivo CSV\n",
    "file_path = 'spotify_songs.csv'  # Reemplaza con la ruta de tu archivo CSV\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Concatenar los campos textuales\n",
    "data['text'] = data['lyrics']\n",
    "\n",
    "# Preprocesar los documentos\n",
    "documents = [tokenize(text) for text in data['text'].dropna()]\n",
    "block_size = 50000  # Ajusta el tamaño del bloque según la memoria disponible\n",
    "\n",
    "# Construcción del índice invertido\n",
    "doc_norms, total_blocks = spimi_invert(documents, block_size)\n",
    "merge_blocks(total_blocks, 'final_inverted_index.txt')\n",
    "\n",
    "# Crear un índice invertido en memoria secundaria para la consulta\n",
    "inverted_index = defaultdict(dict)\n",
    "with open('final_inverted_index.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        term, postings = line.split(': ', 1)\n",
    "        for posting in postings.split():\n",
    "            doc_id, weight = posting.split(':')\n",
    "            inverted_index[term][int(doc_id)] = float(weight)\n",
    "\n",
    "# Proceso de consulta\n",
    "query = \"Oh, thinkin' about our younger years There was only you and me We were young and wild and free Now nothin' can take you away from me We've been down that road before But that's over now You keep me comin' back for more Baby, you're all that I want When you're lyin' here in my arms I'm findin' it hard to believe We're in Heaven And love is all that I need And I found it there in your heart It isn't too hard to see We're in Heaven Oh, once in your life you find someone Who will turn your world around Bring you up when you're feelin' down Yeah, nothin' could change what you mean to me Oh, there's lots that I could say But just hold me now Cause our love will light the way And baby you're all that I want When you're lyin' here in my arms I'm finding it hard to believe We're in Heaven Yeah, love is all that I need And I found it there in your heart It isn't too hard to see We're in Heaven, yeah I've been waitin' for so long For somethin' to arrive For love to come along Now our dreams are comin' true Through the good times and the bad Yeah, I'll be standin' there by you And baby you're all that I want When you're lyin' here in my arms I'm findin' it hard to believe We're in Heaven And love is all that I need And I found it there in your heart It isn't too hard to see We're in Heaven, Heaven, woah You're all that I want You're all that I need\"\n",
    "idf = compute_idf(documents)\n",
    "top_k_results = process_query(query, idf, doc_norms, inverted_index, data)\n",
    "\n",
    "print(\"Top-k documentos relevantes:\")\n",
    "for result in top_k_results:\n",
    "    print(f\"Score: {result['score']}, Track Name: {result['track_name']}, Artist: {result['track_artist']}, Album: {result['track_album_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos tokenizados:\n",
      "Documento 0: ['minsan', 'pa', 'nang', 'ako', 'napalingon', 'hindi', 'ko', 'alam', 'na', 'ika', 'tutugon', 'sa', 'mga', 'tanong', 'na', 'ake', 'nabitawan', 'hindi', 'ko', 'alam', 'kung', 'ito', 'totoo', 'pangarap', 'ka', 'sa', 'bawat', 'sandali', 'langit', 'man', 'ang', 'tingin', 'ko', 'sayo', 'sana', 'marat', 'hanggang', 'dito', 'na', 'lang', 'yata', 'ang', 'kaya', 'kong', 'gawin', 'mangarap', 'na', 'lang', 'bumulong', 'sa', 'hangin', 'kailan', 'kaya', 'darat', 'ulit', 'ang', 'isang', 'sandali', 'na', 'ako', 'lilingon', 'muli', 'pangarap', 'ka', 'tinig', 'mong', 'kay', 'lamig', 'ang', 'iyong', 'mga', 'ngiti', 'na', 'sa', 'akin', 'ay', 'nakapagbigay', 'pansin', 'ikaw', 'ba', 'ay', 'isang', 'pangarap', 'lang', 'pangarap', 'ka', 'tinig', 'mong', 'kay', 'lamig', 'ang', 'iyong', 'mga', 'ngiti', 'na', 'sa', 'akin', 'ay', 'nakapagbigay', 'pangarap', 'ka', 'tinig', 'mong', 'kay', 'lamig', 'ang', 'iyong', 'mga', 'ngiti', 'na', 'sa', 'akin', 'ay', 'nakapagbigay', 'pangarap', 'ka', 'tinig', 'mong', 'kay', 'lamig', 'ang', 'iyong', 'mga', 'ngiti', 'na', 'sa', 'akin', 'ay', 'nakapagbigay', 'pansin']\n",
      "Documento 1: ['tree', 'sing', 'wind', 'sky', 'blue', 'angel', 'smile', 'saw', 'lone', 'bench', 'half', 'past', 'four', 'kiss', 'soft', 'soft', 'hand', 'kiss', 'lip', 'angel', 'smile', 'thought', 'hey', 'feel', 'aliv', 'park', 'sign', 'said', 'close', 'jump', 'fenc', 'care', 'kiss', 'tree', 'danc', 'midnight', 'sun', 'love', 'without', 'know', 'laugh', 'felt', 'free', 'angel', 'smile', 'thought', 'hey', 'feel', 'aliv']\n",
      "Documento 2: ['na', 'yeah', 'spyderman', 'freez', 'full', 'effect', 'readi', 'ron', 'readi', 'readi', 'biv', 'readi', 'slick', 'oh', 'yeah', 'break', 'na', 'girl', 'must', 'warn', 'sens', 'someth', 'strang', 'mind', 'situat', 'serious', 'let', 'cure', 'caus', 'run', 'time', 'oh', 'beauti', 'relationship', 'seem', 'start', 'dead', 'love', 'togeth', 'heart', 'drivin', 'mind', 'hard', 'find', 'ca', 'get', 'head', 'miss', 'kiss', 'love', 'wrong', 'move', 'dead', 'girl', 'poison', 'never', 'trust', 'big', 'butt', 'smile', 'girl', 'poison', 'poison', 'na', 'start', 'meet', 'fli', 'girl', 'know', 'caus', 'portion', 'think', 'best', 'thing', 'world', 'fli', 'drive', 'right', 'mind', 'steal', 'heart', 'blind', 'bewar', 'schemin', 'make', 'think', 'dreamin', 'fall', 'love', 'screamin', 'demon', 'hoo', 'poison', 'dead', 'movin', 'slow', 'lookin', 'mellow', 'fellow', 'like', 'devo', 'gettin', 'paid', 'laid', 'better', 'lay', 'low', 'schemin', 'hous', 'money', 'whole', 'show', 'low', 'pro', 'ho', 'cut', 'like', 'see', 'sayin', 'huh', 'winner', 'know', 'loser', 'know', 'crew', 'use', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'poison', 'park', 'shake', 'breakin', 'takin', 'night', 'play', 'wall', 'checkin', 'fella', 'high', 'low', 'keepin', 'one', 'eye', 'open', 'still', 'clockin', 'hoe', 'one', 'particular', 'girl', 'stood', 'rest', 'poison', 'high', 'power', 'chest', 'michael', 'biv', 'runnin', 'show', 'bell', 'biv', 'devo', 'know', 'yo', 'slick', 'blow', 'drivin', 'mind', 'hard', 'find', 'ca', 'get', 'head', 'miss', 'kiss', 'love', 'wrong', 'move', 'dead', 'girl', 'poison', 'never', 'trust', 'big', 'butt', 'smile', 'girl', 'poison', 'poison', 'yo', 'fella', 'end', 'know', 'sayin', 'mike', 'yeah', 'full', 'effect', 'yo', 'wassup', 'ralph', 'johnni', 'g', 'ca', 'forget', 'boy', 'brown', 'whole', 'ne', 'crew', 'poison', 'na']\n",
      "Documento 3: ['realli', 'ca', 'stay', 'babi', 'cold', 'outsid', 'got', 'go', 'away', 'babi', 'cold', 'even', 'hope', 'drop', 'nice', 'hold', 'hand', 'like', 'ice', 'mother', 'start', 'worri', 'beauti', 'hurri', 'father', 'pace', 'floor', 'listen', 'fireplac', 'roar', 'realli', 'better', 'scurri', 'beauti', 'pleas', 'hurri', 'well', 'mayb', 'half', 'drink', 'put', 'record', 'pour', 'neighbor', 'might', 'think', 'babi', 'bad', 'say', 'drink', 'cab', 'wish', 'knew', 'eye', 'like', 'starlight', 'break', 'spell', 'take', 'hat', 'hair', 'look', 'swell', 'ought', 'say', 'sir', 'mind', 'move', 'closer', 'least', 'gon', 'na', 'say', 'tri', 'sens', 'hurt', 'pride', 'realli', 'ca', 'stay', 'babi', 'hold', 'babi', 'cold', 'outsid', 'simpli', 'must', 'go', 'see', 'cold', 'outsid', 'answer', 'said', 'cold', 'welcom', 'lucki', 'drop', 'nice', 'warm', 'look', 'window', 'storm', 'sister', 'suspici', 'gosh', 'lip', 'look', 'delici', 'brother', 'door', 'wave', 'upon', 'tropic', 'shore', 'maiden', 'aunt', 'mind', 'vicious', 'oh', 'lip', 'delici', 'mayb', 'cigarett', 'never', 'blizzard', 'hey', 'got', 'go', 'home', 'babi', 'freez', 'say', 'lend', 'coat', 'knee', 'realli', 'grand', 'thrill', 'touch', 'hand', 'see', 'thing', 'bound', 'talk', 'tomorrow', 'think', 'life', 'long', 'sorrow', 'least', 'plenti', 'impli', 'caught', 'pneumonia', 'die', 'realli', 'ca', 'stay', 'get', 'old', 'lie', 'babi', 'babi', 'cold', 'outsid']\n",
      "Documento 4: ['get', 'busi', 'keep', 'turn', '모두', '다', '여긴', 'wit', '넌', '바른', '척하는', 'crimin', '찔리는', '걸', 'feel', '너', '네가', '뭔데', '왜', '솔직하지', '못해', '넌', '다를', '건', '대체', '뭔데', 'aye', 'bad', 'boy', '그래', '난', 'bad', 'boy', '생각해', '네가', '하고픈', '대로', 'boy', '그래', '난', 'boy', '모르면', '믿고', '싶은', '대로', 'noth', 'bout', 'nada', 'know', 'noth', 'bout', 'thing', '욕을', '하는', '이윤', '다', '있는', '거야', '나', '아닌', '너', '안에', 'caus', 'parti', 'yeah', 'parti', 'worri', 'bout', 'drama', 'bodi', 'everybodi', '우린', '아침까지', 'wild', '신경', '쓸건', '따로', '아', '몰라', '오늘', '다', '집어치워', '눈치', '보지', '말고', 'volum', 'go', 'dumb', 'dumb', 'dumb', 'dumb', '누가', '뭐래도', 'litti', 'dumb', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'stupid', 'stupid', 'dumb', 'dumb', 'dumb', 'dumb', '누가', '뭐래도', 'litti', 'dumb', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'go', '몰입하니', '눈', '몰리는', '이치', '시시콜콜', '무료함을', '완충해', '내키면', '곧장', 'dumb', 'litti', '이', '노래가', '흐름', 'come', 'get', 'drippin', '여기', '주도권을', 'grip', '거리낌', '허물어', 'let', 'go', 'parti', '돌변함은', '때', '때에', '따라', '아무렴', '어때', '까무러치게', '더불어', '즐겨', '사는', '거지', 'way', 'follow', 'nobodi', 'parti', 'yeah', 'parti', 'worri', 'bout', 'drama', 'bodi', 'everybodi', '우린', '아침까지', 'wild', '신경', '쓸건', '따로', '아', '몰라', '오늘', '다', '집어치워', '터지게', '더', '올려', 'volum', 'go', 'dumb', 'dumb', 'dumb', 'dumb', '누가', '뭐래도', 'litti', 'dumb', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'stupid', 'stupid', 'dumb', 'dumb', 'dumb', 'dumb', '누가', '뭐래도', 'litti', 'dumb', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'go', '숨이', '차올라', '내', '심장을', '죄여', '이젠', '풀어', '나의', '사슬', 'wake', '오늘', '밤', '지나', '해가', '뜰', '때까지', '계속', 'go', 'dumb', '내일', '일은', '내일', 'uh', '끝이', '난', '듯', '저질러', 'go', 'dumb', 'dumb', 'dumb', 'dumb', '누가', '뭐래도', 'litti', 'dumb', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'stupid', 'stupid', 'dumb', 'dumb', 'dumb', 'dumb', '누가', '뭐래도', 'litti', 'dumb', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'go', 'litti', 'dumb', 'litti', 'bang', 'drum', '갈', '때까지', 'babi', 'go']\n",
      "Pesos TF-IDF para el primer documento: {'minsan': 0.061669905224261176, 'pa': 0.022096187788830445, 'nang': 0.05365872157173686, 'ako': 0.09711291672177735, 'napalingon': 0.0754526703721693, 'hindi': 0.09964065597241235, 'ko': 0.13473589003453584, 'alam': 0.11267600767067704, 'na': 0.04582195283268307, 'ika': 0.05945696620540132, 'tutugon': 0.0754526703721693, 'sa': 0.2883192228069192, 'mga': 0.2534604355197618, 'tanong': 0.06048413076405149, 'ake': 0.05203326700506603, 'nabitawan': 0.0754526703721693, 'kung': 0.04982032798620618, 'ito': 0.055722290699388236, 'totoo': 0.06478886759432398, 'pangarap': 0.37843427242836497, 'ka': 0.20730794386866958, 'bawat': 0.06478886759432398, 'sandali': 0.12614475747612167, 'langit': 0.05700732211987413, 'man': 0.013083276361578928, 'ang': 0.32319455854062934, 'tingin': 0.06478886759432398, 'sayo': 0.06048413076405149, 'sana': 0.04727142694040278, 'marat': 0.0754526703721693, 'hanggang': 0.05855094285419837, 'dito': 0.05945696620540132, 'lang': 0.12438476632120175, 'yata': 0.06700180661318383, 'kaya': 0.11267600767067704, 'kong': 0.0455155911559337, 'gawin': 0.07012076898324664, 'mangarap': 0.0754526703721693, 'bumulong': 0.0754526703721693, 'hangin': 0.03696077571105037, 'kailan': 0.06307237873806083, 'darat': 0.06700180661318383, 'ulit': 0.06700180661318383, 'isang': 0.11030445875025767, 'lilingon': 0.0754526703721693, 'muli': 0.06700180661318383, 'tinig': 0.28048307593298655, 'mong': 0.21287616586110286, 'kay': 0.22288916279755294, 'lamig': 0.2591554703772959, 'iyong': 0.22060891750051534, 'ngiti': 0.25228951495224333, 'akin': 0.23096190939655267, 'ay': 0.1373277893593269, 'nakapagbigay': 0.3018106814886772, 'pansin': 0.14024153796649327, 'ikaw': 0.054621514979075365, 'ba': 0.04299491879389154}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m tf_idf_example \u001b[38;5;241m=\u001b[39m {term: tf_example[term] \u001b[38;5;241m*\u001b[39m idf_example[term] \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m tf_example}\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPesos TF-IDF para el primer documento: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf_idf_example\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m \u001b[43mtf_idf_query\u001b[49m\n\u001b[0;32m     11\u001b[0m doc_vector \u001b[38;5;241m=\u001b[39m inverted_index[tokens[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     12\u001b[0m doc_norm \u001b[38;5;241m=\u001b[39m doc_norms[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_idf_query' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
