{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class SPIMIIndexer:\n",
    "    def __init__(self, csv_path, block_size=5000, temp_dir='temp_blocks', final_index_file='final_index.pkl'):\n",
    "        # Leer el archivo CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # Obtener las letras de las canciones\n",
    "        self.lyrics = self.df['lyrics'].fillna('').tolist()\n",
    "        # Obtener metadatos de las canciones\n",
    "        self.song_metadata = self.df[['track_name', 'track_artist', 'track_album_name']].to_dict('records')\n",
    "        self.block_size = block_size\n",
    "        self.temp_dir = temp_dir\n",
    "        self.final_index_file = final_index_file\n",
    "        self.num_docs = len(self.lyrics)\n",
    "        self.stop_words = set(stopwords.words('spanish')).union(set(stopwords.words('english')))\n",
    "        \n",
    "        # Crear el directorio temporal si no existe\n",
    "        if not os.path.exists(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "        \n",
    "        # Crear el índice invertido en bloques y luego fusionar los bloques\n",
    "        self.spimi_invert()\n",
    "        self.merge_blocks()\n",
    "        self.cleanup_temp_files()\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        # Tokenización, eliminación de stopwords y stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered_tokens = [stemmer.stem(word) for word in tokens if word.isalnum() and word not in self.stop_words]\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def spimi_invert(self):\n",
    "        block_id = 0\n",
    "        # Procesar los documentos en bloques\n",
    "        for i in range(0, len(self.lyrics), self.block_size):\n",
    "            block = self.lyrics[i:i + self.block_size]\n",
    "            dictionary = defaultdict(list)\n",
    "            doc_norms = defaultdict(float)\n",
    "            \n",
    "            for doc_id, text in enumerate(block):\n",
    "                tokens = self.preprocess(text)\n",
    "                term_freq = Counter(tokens)\n",
    "                \n",
    "                for term, freq in term_freq.items():\n",
    "                    dictionary[term].append((i + doc_id, freq))\n",
    "                    doc_norms[i + doc_id] += (freq ** 2)\n",
    "            \n",
    "            for doc_id in doc_norms:\n",
    "                doc_norms[doc_id] = np.sqrt(doc_norms[doc_id])\n",
    "            \n",
    "            # Guardar el bloque en un archivo temporal\n",
    "            block_path = os.path.join(self.temp_dir, f'block_{block_id}.pkl')\n",
    "            with open(block_path, 'wb') as f:\n",
    "                pickle.dump((dictionary, doc_norms), f)\n",
    "            \n",
    "            block_id += 1\n",
    "    \n",
    "    def merge_blocks(self):\n",
    "        block_files = [os.path.join(self.temp_dir, f) for f in os.listdir(self.temp_dir)]\n",
    "        heap = []\n",
    "        term_postings = defaultdict(list)\n",
    "        doc_norms = defaultdict(float)\n",
    "        \n",
    "        for block_file in block_files:\n",
    "            with open(block_file, 'rb') as f:\n",
    "                dictionary, block_doc_norms = pickle.load(f)\n",
    "                for term, postings in dictionary.items():\n",
    "                    for posting in postings:\n",
    "                        heapq.heappush(heap, (term, posting))\n",
    "                for doc_id, norm in block_doc_norms.items():\n",
    "                    if doc_id in doc_norms:\n",
    "                        doc_norms[doc_id] += norm ** 2\n",
    "                    else:\n",
    "                        doc_norms[doc_id] = norm ** 2\n",
    "        \n",
    "        while heap:\n",
    "            term, posting = heapq.heappop(heap)\n",
    "            term_postings[term].append(posting)\n",
    "        \n",
    "        for doc_id in doc_norms:\n",
    "            doc_norms[doc_id] = np.sqrt(doc_norms[doc_id])\n",
    "        \n",
    "        # Guardar el índice final en un archivo\n",
    "        with open(self.final_index_file, 'wb') as f:\n",
    "            pickle.dump((term_postings, doc_norms), f)\n",
    "        \n",
    "        self.dictionary = term_postings\n",
    "        self.doc_norms = doc_norms\n",
    "    \n",
    "    def cleanup_temp_files(self):\n",
    "        # Eliminar los archivos temporales\n",
    "        for block_file in os.listdir(self.temp_dir):\n",
    "            os.remove(os.path.join(self.temp_dir, block_file))\n",
    "        os.rmdir(self.temp_dir)\n",
    "    \n",
    "    def load_final_index(self):\n",
    "        # Cargar el índice final desde el archivo\n",
    "        with open(self.final_index_file, 'rb') as f:\n",
    "            self.dictionary, self.doc_norms = pickle.load(f)\n",
    "    \n",
    "    def compute_tfidf(self, term, doc_id):\n",
    "        # Calcular el peso TF-IDF para un término en un documento\n",
    "        term_postings = self.dictionary.get(term, [])\n",
    "        doc_freq = len(term_postings)\n",
    "        tf = next((freq for doc, freq in term_postings if doc == doc_id), 0)\n",
    "        idf = np.log(self.num_docs / (1 + doc_freq))\n",
    "        return tf * idf\n",
    "    \n",
    "    def cosine_similarity(self, query):\n",
    "        # Preprocesar la consulta\n",
    "        query_tokens = self.preprocess(query)\n",
    "        query_vector = Counter(query_tokens)\n",
    "        \n",
    "        # Normalizar el vector de la consulta\n",
    "        query_tfidf_vector = {}\n",
    "        query_norm = 0\n",
    "        for term, count in query_vector.items():\n",
    "            if term in self.dictionary:\n",
    "                idf = np.log(self.num_docs / (1 + len(self.dictionary[term])))\n",
    "                query_tfidf = count * idf\n",
    "                query_tfidf_vector[term] = query_tfidf\n",
    "                query_norm += query_tfidf ** 2\n",
    "        \n",
    "        query_norm = np.sqrt(query_norm)\n",
    "        if query_norm == 0:\n",
    "            query_norm = 1  # para evitar la división por cero\n",
    "        \n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        # Calcular la similitud de coseno entre la consulta y los documentos\n",
    "        for term, query_tfidf in query_tfidf_vector.items():\n",
    "            query_tfidf /= query_norm\n",
    "            if term in self.dictionary:\n",
    "                for doc_id, freq in self.dictionary[term]:\n",
    "                    doc_tfidf = self.compute_tfidf(term, doc_id) / self.doc_norms[doc_id]\n",
    "                    scores[doc_id] += query_tfidf * doc_tfidf\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def retrieve_top_k(self, query, k=5, additional_features=None):\n",
    "        if additional_features is None:\n",
    "            additional_features = []\n",
    "        \n",
    "        self.load_final_index()  # Cargar el índice final para consultas\n",
    "        \n",
    "        start_time = time.time()\n",
    "        scores = self.cosine_similarity(query)\n",
    "        sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_k_results = sorted_scores[:k]\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in top_k_results:\n",
    "            metadata = self.song_metadata[doc_id]\n",
    "            result = {\n",
    "                'track_name': metadata['track_name'],\n",
    "                'row_position': doc_id,\n",
    "                'cosine_similarity': score\n",
    "            }\n",
    "            for feature in additional_features:\n",
    "                if feature in self.df.columns:\n",
    "                    result[feature] = self.df.iloc[doc_id][feature]\n",
    "            results.append(result)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Retornar un diccionario con el tiempo total de la consulta y los resultados\n",
    "        return {\n",
    "            'query_time': end_time - start_time,\n",
    "            'results': results\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso\n",
    "indexer = SPIMIIndexer('spotify_songs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'track_name': 'Madura (feat. Bad Bunny)', 'row_position': 1668, 'cosine_similarity': 1.8831536805417297, 'track_artist': 'Cosculluela', 'track_album_name': 'Madura (feat. Bad Bunny)'}, {'track_name': 'La Jeepeta', 'row_position': 11175, 'cosine_similarity': 0.7053760357312566, 'track_artist': 'Nio Garcia', 'track_album_name': 'La Jeepeta'}, {'track_name': 'Mi Dios Es Grande', 'row_position': 6393, 'cosine_similarity': 0.6949870140405484, 'track_artist': 'Artury Pepper', 'track_album_name': 'Mi Dios Es Grande'}, {'track_name': 'Ocean', 'row_position': 15815, 'cosine_similarity': 0.49552194148059103, 'track_artist': 'KAROL G', 'track_album_name': 'OCEAN'}, {'track_name': 'Superhuman (feat. Eric Leva)', 'row_position': 12837, 'cosine_similarity': 0.4361188230920928, 'track_artist': 'SLANDER', 'track_album_name': 'Superhuman (feat. Eric Leva)'}, {'track_name': 'Daylight', 'row_position': 5749, 'cosine_similarity': 0.4281809558551308, 'track_artist': 'Matt and Kim', 'track_album_name': 'Grand'}, {'track_name': 'Daylight', 'row_position': 12075, 'cosine_similarity': 0.4281809558551308, 'track_artist': 'Matt and Kim', 'track_album_name': 'Grand'}, {'track_name': 'McDonalds Rich', 'row_position': 4410, 'cosine_similarity': 0.406179946705751, 'track_artist': 'SAINt JHN', 'track_album_name': 'McDonalds Rich'}, {'track_name': 'Clandestino (feat. Calypso Rose)', 'row_position': 2150, 'cosine_similarity': 0.4032207646336693, 'track_artist': 'Manu Chao', 'track_album_name': 'Clandestino (feat. Calypso Rose)'}, {'track_name': 'Asesino de Acesinos', 'row_position': 5792, 'cosine_similarity': 0.4012509396396048, 'track_artist': 'Cartel De Santa', 'track_album_name': 'Cartel De Santa'}]\n"
     ]
    }
   ],
   "source": [
    "# Consulta de ejemplo\n",
    "query = \"Pero tú 'tás grande, 'tá madura\"\n",
    "additional_features = ['track_artist', 'track_album_name']\n",
    "top_k_results = indexer.retrieve_top_k(query, k=10, additional_features=additional_features)\n",
    "\n",
    "print(top_k_results[\"results\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
