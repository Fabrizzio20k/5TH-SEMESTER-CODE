{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vilch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3- Matriz de similitudes\n",
    "### Elabore una matriz de similitud de coseno entre los documentos de la colección \"El Señor de los Anillos\". Debe aplicar los pesos TF-IDF.\n",
    "### 1. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento(texto):\n",
    "    # tokenizar\n",
    "\n",
    "    words = nltk.word_tokenize(texto)\n",
    "\n",
    "    # filtrar caracteres especiales\n",
    "\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "  \n",
    "    # filtrar stopwords\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "    words = [word for word in words if word.lower() not in stopwords]\n",
    "    \n",
    "    # reducir palabras\n",
    "\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "textos = [\"libro1.txt\",\"libro2.txt\",\"libro3.txt\",\"libro4.txt\",\"libro5.txt\",\"libro6.txt\"]\n",
    "textos_procesados = []\n",
    "indice = {}\n",
    "for file_name in textos:\n",
    "  file = open(file_name, encoding=\"utf-8\")\n",
    "  texto = file.read().rstrip()\n",
    "  texto = preprocesamiento(texto)  \n",
    "  textos_procesados.append(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Similitud de coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 0.54 0.41 0.48 0.36 0.59 \n",
      "0.54 1.00 0.39 0.44 0.37 0.54 \n",
      "0.41 0.39 1.00 0.18 0.40 0.34 \n",
      "0.48 0.44 0.18 1.00 0.30 0.59 \n",
      "0.36 0.37 0.40 0.30 1.00 0.34 \n",
      "0.59 0.54 0.34 0.59 0.34 1.00 \n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(collection):\n",
    "    index = {}\n",
    "    for i, doc in enumerate(collection):\n",
    "        for word in doc:\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            index[word].append(i)\n",
    "\n",
    "    tfidf = []\n",
    "    for doc in collection:\n",
    "        doc_tfidf = []\n",
    "        for word in index.keys():\n",
    "            tf = doc.count(word)\n",
    "            idf = np.log(len(collection) / len(index[word]))\n",
    "            doc_tfidf.append(tf * idf)\n",
    "        tfidf.append(doc_tfidf)\n",
    "    return tfidf\n",
    "\n",
    "def cosine_sim(Q, Doc):  \n",
    "    Q = np.array(Q)\n",
    "    Doc = np.array(Doc)\n",
    "    return np.dot(Q, Doc) / (np.linalg.norm(Q) * np.linalg.norm(Doc))\n",
    "  \n",
    "textos_tfidf = compute_tfidf(textos_procesados)\n",
    "\n",
    "def print_pretty_matrix(matrix):\n",
    "  for row in matrix:\n",
    "    for value in row:\n",
    "      print(\"{:.2f}\".format(value), end=\" \")\n",
    "    print()\n",
    "\n",
    "matriz = []\n",
    "for doc1 in textos_tfidf:\n",
    "  row = []\n",
    "  for doc2 in textos_tfidf:  \n",
    "    row.append(cosine_sim(doc1, doc2))\n",
    "  matriz.append(row)\n",
    "\n",
    "print_pretty_matrix(matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4- Indice invertido con similitud de coseno\n",
    "\n",
    "### 1. Estructura del índice invertido en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class InvertIndex: \n",
    "\n",
    "    def __init__(self, index_file):\n",
    "        self.index_file = index_file\n",
    "        self.index = {}\n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "\n",
    "    \n",
    "    def __load_index(self, index_file):\n",
    "        self.index = json.load(open(index_file))\n",
    "\n",
    "    def __save_index(self, index_file):\n",
    "        json.dump(self.index, open(index_file, 'w'))\n",
    "\n",
    "    def building(self, collection_text):\n",
    "        # build the inverted index with the collection\n",
    "        # compute the tf\n",
    "        # compute the idf\n",
    "        # compute the length (norm)\n",
    "        # store in disk\n",
    "        pass\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        self.__load_index(self.index_file)\n",
    "        # diccionario para el score\n",
    "        score = {}\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        \n",
    "        # calcular el tf-idf del query\n",
    "        \n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "                \n",
    "        # ordenar el score de forma descendente\n",
    "        result = sorted(score.items(), key= lambda tup: tup[1], reverse=True)\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        return result[:k]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2:\tProbar el Índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m Query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEl regulador de valores de China\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m result \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mretrieval(Query1, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataton = pd.read_csv('df_total.csv')\n",
    "\n",
    "collection_text = dataton['news']\n",
    "index = InvertIndex(\"index.txt\")\n",
    "index.building(collection_text)\n",
    "\n",
    "Query1 = \"El regulador de valores de China\"\n",
    "result = index.retrieval(Query1, 10)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
